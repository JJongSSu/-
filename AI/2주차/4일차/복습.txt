교차검증
    학습-평가 data를 골고루 설정하여 모델의 안정성을 높이고 과대적합을 감소시키는 통계적 기법
    같은 data 내에서 학습, 평가 data를 나누기 때문에 검증에 충분하지 않다

holdout
    학습, 검증, 평가 data를 각각 만들어 2번 검증

k-fold
    1개의 data set을 k개로 나눠 k-1개 data set을 학습시킴 -> 나머지 1개의 data set으로 평가 -> k번 수행 -> 각 평가결과의 평균 구함

장단점
    모든 data를 사용하기 때문에 과대적합 감소 -> 계산량 증가 -> 시간, 비용 증가
    모델의 data의 변경에 대한 민감도 파악 가능
    