<로지스틱 회귀(Logistic Regression)> 
분류 모델
회귀와 분류를 동시에 사용하기 위해 로지스틱 회귀 사용
    (회귀모델을 짠다음 로스스틱 회귀를 통해 분류하도록 하는 방법)

선형모델 방식을 분류에 사용
    선형모델은 간단한 함수식을 사용하기 때문에 학습 및 예측 속도가 빠름
    큰 data set에도 잘 동작
    일반적으로 특성이 많을수록 더 잘 동작

선형회귀 직선을 통해 두 집단을 나눌 수 있다

Sigmoid 함수 -> 모든 분류 값을 0~1 사이의 확률 정보로 표시 가능
    y = 1 / (1 + e^(-x))
    0.5 기준으로 낮으면 0, 높으면 1로 예측 -> 몇 %로 0인지, 1인지 판단 가능
        x=0, 0.5
        x=무한대, 1
        x=-무한대, 0
    -> 어떤 회귀선이든 0~1사이 값을 가짐, 중심은 50%로 수렴
    -> 회귀 모델을 분류모델로 만들 수 있음

    단점 : 2진 분류, 레이블이 2개일 때만 사용 가능
        2개를 넘어가면 레이블의 범위가 달라지기 때문에 공평하지 않음
        양쪽 끝은 무한대로 수렴하기 때문
    
    보완방법(다진분류)
        <lib 사용 가능>
        - Softmax
            (ei)^x / ((e1)^x+(e2)^x+(e3)^x+...+(ek)^x)
            e = 레이블의 개수
            i = 해당 레이블
            해당 레이블에 할당될 확률을 구하는 방법, 0~1 사이값
        
        <직접코딩>
        - OVO
            이진분류 model을 여러 개 만들어 리그형식으로 매치시키는 방법
            중복이 되면 많이 나온 결과값으로 결정(A,A,B -> A)
                model 1 = A vs B
                model 2 = B vs C
                model 1 = A vs C
            Softmax보다 성능이 좋음
            분류모델은 레이블 개수가 많아질수록 성능이 떨어짐
            여러 개의 model을 돌려야 되기 때문에 시간이 오래걸림

        - OVR
            여집합 사용
            먼저 model 1을 돌려 A를 뽑아내고 나머지 여집합 중에서 다시 model 2를 돌려 B를 뽑아내는 방식
                model 1 = A vs A^c
                model 2 = B vs B^c
            OVO보다 계산 시간 빠름, 겹치지 않음
            data 불균형이 발생함 -> model이 한쪽으로 밀어버릴 수 있음
                (A B C가 각각 30 30 30이면 30:60이 되기 때문)