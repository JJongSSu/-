머신러닝 : 지도학습, 비지도학습

문제정의
데이터 전처리
모델선언
모델학습
모델예측

데이터의 분포에 따라서 머신러닝, 딥러닝 모델을 선택

기하모델 -> 단위 스케일링이 중요함

k-means -> 무게중심을 계산, 이상치에 영향을 받음(이상치가 많으면 사용하기 어려움)
DBSCAN -> 밀도를 계산, 원으로 분포되어 있는 모양에 사용
계층적 군집분석

머신러닝은 특성의 개수가 유한함
딥러닝은 명확한 특성으로 나타낼 수 없다, 특성의 개수가 무한

텍스트 마이닝 
    자연어를 숫자로 치환하는 과정(임베딩)
        count-based
            tokenizing을 통해 나온 token들에 숫자를 매겨 텍스트를 숫자로 치환하는 방법
            
            BoW(bag of words) : 텍스트를 자연수로 치환
            원핫인코딩 : 텍스트를 2진수로 치환
            -> 간단하고 비용이 적게 들음
            -> 동음이의어, 다의어를 구분할 수 없다
            -> 단어의 유사성을 파악할 수 없다
        
        distributed
            단어간의 유사성을 파악할 수 없는 count-based의 단점을 보완하기 위해 만듬
            텍스트를 행렬로 치환
            각각의 의미에 대한 벡터들을 행렬로 치환
            의미는 단어의 호환관계로 추론
            행렬이기 때문에 단어간의 계산이 가능
            
            W2V : 단어들의 벡터 추론 가능, 학습하지 않은 단어에 대해서는 예측이 어려움(호환관계를 관찰한 적이 없기 때문)
            D2V : 문장, 문단의 벡터도 추론 가능
                문장의 끝에 가짜 token을 넣어서 document 구분
                -> 번역기 : document 행렬간 의미가 비슷하면 번역
                -> 문장을 만드는 것은 딥러닝으로 돌림
            fast 텍스트(페이스북에서 만듬)
                단어들의 스펠링 묶음을 행렬로 치환해서 단어를 치환 
                신조어에 대응하기 위해서 나옴
                학습하지 않은 단어들도 추론 가능

        contectionalized -> 이미 학습된 모델을 뿌림(개인 pc에서 학습시킬 수 없음)
            동음이의어, 다의어를 구분할 수 없는 count-based의 단점을 보완하기 위해 만듬
            단어가 나올만한 공간을 임베딩 -> 공간에 숫자를 할당
            의미 벡터가 공간에 오는 것이 중요
            
            Elmo : '배가 아파'에서 '배가 뭐야'라고 물어봐야 함, 배의 위치로 파악하기 때문에
            GPT
            BERT(구글) : 구글에 있는 모든 단어를 학습시킴

딥러닝
    1. 문제정의
    2. 데이터 전처리 : 임베딩(벡터화), 스케일링, 패딩 -> 스스로 하기 때문에 전처리 안해도 됨

    특징
        활성화함수 => 비선형 함수 사용
        1차 함수(선형함수)를 사용하면 layer를 여러개 쌓아도(다중 layer) 딥러닝의 효율이 떨어지기 때문에 사용안함

    문제정의 : 텍스트 긍정/부정 -> 전처리(임베딩) : W2V -> 모델 선언 -> 학습 -> 예측
    모델 학습 : 순전파, 역전파
    경사하강법 : 계산량이 많아지면 노가다하는게 더 빠를 수 있기 때문에
    모든 단어들을 표현하기 위해서는 차원이 많아진다
    선형대수를 활용하여 최적의 파라미터를 뽑아내는 것이 딥러닝이다

    데이터를 미지의 함수에 넣어 결과를 냄 -> 결과를 평가 -> 함수 수정 이 단계를 무한히 반복
        미지의 함수 = 활성화 함수(activation function)
        결과 = 손실함수(loss function)  ex) mse = (측정값-예측값)^2
        학습률(learning rate) = 옵티마이져(optimizer) : 손실함수를 평가한 후 얼마나 조정할지 결정

        옵티마이져에서 결정된 숫자를 기반으로 파라미터를 변경 : 역전파

    epoch : 순전파, 역전파 1회씩
    = max_iter : 몇 번의 학습을 시킬 것인지

    딥러닝 과정 : input layer - hidden layer (알고리즘 : 활성화 함수) - output layer
        input layer
            행렬 크기는 모두 같아야 함

        hidden layer
            tanh : 원점에 대하여 점대칭, 음수의 개념이 중요할 때 사용(특성의 구분에 음수가 중요한 역할을 할 때)
            ReLU : 0 미만이면 수렴, 성능 좋음

        output layer에서 활성화함수는 회귀모델은 linear 함수 사용, 이진분류 모델은 시그모이드 함수 사용, 다진분류 모델은 소프트맥스 함수 사용
            시그모이드 단점 : 2진분류만 가능 -> 소프트맥스 사용
            결과 개수
                회귀모델 : 연속형 변수 1개
                이진분류 모델 : 1개
                다진분류 모델 : 클래스 개수만큼 나옴(각 클래스의 할당될 확률로 결과가 나옴, 각 확률의 합은 1)

        손실함수 
            낮을수록 학습이 잘 된 거임
            예측값과 실제값을 비교
            
            회귀 : mse
            2진 분류 : binary_crossentropy -> R^2 원리와 비슷
            다중 분류 : categorical_crossentropy

        optimizer
            손실함수의 값을 어떤 방향으로 얼마나 움직일지를 구해주는 것
            얼마나 움직일지 = learning rate : 사람이 정함
            딥러닝에서 방향만 결정해주면 됨

            1. momentum(관성)
                이전에 설정했던 방향과 optimizer가 설정한 방향의 벡터값으로 움직임
                속도가 빠름, 갈수록 화살표의 길이가 늘어남 = 힘의 세기가 커진다(억제가 필요)

            2. 시계열
                추세 : 장기적 변화, 주기 : 단기적 변화
                순서가 있을 때 사용가능
                순서가 있지만 시간에는 따르지 않는 것 : 언어, epoch(이전 epoch를 통해 다음 epoch를 예측)
                시계열을 통해 다음 epoch의 경사하강을 예측
                한번 잘못 설정되면 수렴을 못할 수 있음

            Adam : momentum + RMSProp(시계열 모델)

    별다른 이유 없이 ReLU, Adam을 사용하는 문서는 의심필요
    잘 모르겠으면 ReLU, Adam 사용하면 된다. 대신 사용 이유를 적어줘야 함

딥러닝 -> 과적합에 민감 -> 하나의 특성에 의존하는 경향
과적합 방지
    규제
        특성에 상한선을 정해서 의존하지 못하게 만듬 -> Ridge, Lasso
        특성을 하나씩 빼면서 학습시킴 -> drop out

1. gradient 소실
    tanh, sigmoid
    x값의 가중치에 대한 y값의 변화량이 작아짐 -> 학습효과가 없어짐 -> gradient 소실

2. gradient 클리핑 : hyper parameter(필수 x)
    x값을 -1~1로 제한, 범위 밖의 값은 -1 or 1로 수렴시킴
    범위는 hyper parameter로 숫자를 정할 수 있음
    ex) -1~1, -2~2...

3. 초기화(initializeation) : 필수
    학습했던 내용을 0으로 초기화하는 것이 아니라 input, output 가중치의 평균값으로 초기화 
    과대적합 방지를 위해 사용
    0으로 초기화하면 학습하는 의미가 없음 -> 랜덤으로 파라미터를 정하기 때문

4. batch size

5. drop out
    전체 뉴런 중에서 제외할 뉴런 결정

