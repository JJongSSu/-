데이터                  input layer
전처리(임베딩)
학습                    hidden layer    ->  활성화 함수 : tanh, ReLU
출력                    output layer    -> 회귀 : linear, 이진분류 : sigmoid, 다진분류 : softmax
평가                    loss function   -> 회귀 : mse, 이진분류 : binary_crossentropy, 다진분류 : categorical_crossentropy 
조정, 최적화            optimizer       -> 벡터(정도와 방향) : 경사하강을 얼만큼? 어디 방향으로? / 방법 : 시계열(주기, 추세) -> 패턴(힘의 크기), 관성(momentum) -> 방향성
역전파

딥러닝 특징
    데이터 양 많아야 함 -> 경사하강법 사용
    성능은 좋지만 판단근거를 추적할 수 없음 -> feature 개수가 무한하기 때문

손실함수에서 최솟값을 찾는 방법 : 미분, 경사하강법(연산량이 많을 때)

momentum -> 갈수록 힘의 크기가 쎄짐
Scheduler
    optimizer가 갈수록 보폭을 줄여가는 방법, epoch에서 갈수록 learning data를 줄여나감

CNN -> YOLO
    7*7 이미지 -> 3*3 filter(hidden layer) -> 차원 축소 : 5*5 -> 3*3 filter(hidden layer) -> 차원 축소 : 3*3 -> 3*3 filter(hidden layer) -> 1차원(1개의 값) : 사과다 / 아니다
    input data -> 전처리(padding/자르기) -> hidden layer -> ouput layer -> 손실함수 -> 옵티마이저

    필터링(filter)
        1. 차원 축소
        2. 활성화 함수를 사용하여 맞춰야 함
    
    pooling
        filter의 값들을 한 개의 특정값(평균치 값)으로 치환하는 것 -> 차원 축소

    어느 부분이 중요한지 표시할 수 없음

    마지막 결과 값을 회귀로도 표시할 수 있음
    여러 특성을 나타내게 할 수도 있음 -> 행렬로 표시
    
    GPU 연산 -> 2차원, CPU 연산 -> 1차원
    CNN은 2차원이기 때문에 GPU로 연산 가능

    이미지의 크기가 모두 동일해야 사용가능
    padding(패딩)
        크기가 일정하지 않은 이미지들의 크기를 모두 일정하게 만드는 작업
        크기가 작은 이미지에 0을 넣어줌
        입력과 출력의 크기를 같게 맞춰줄 수 있음
        층이 깊어지면서 이미지 크기가 줄어드는 것을 방지
        Conv2D 계층에서는 padding 명령을 사용해 패딩을 지정할 수 있음

        -> CNN이 차원을 축소하려는건데 왜 패딩을 해서 다시 차원을 늘려주지??
            input data의 크기가 모두 같을수가 없으니 크기를 맞춰줘야 함
            input data에만 패딩을 사용해서 차원크기를 일정하게 해줌(전처리 의미로 생각하면 좋음)

    stride
        filtering할 때 몇 칸씩 이동할지 결정하는 단위

    RGB -> 256개 3차원 임베딩 가능
    일반적으로 색깔층과 차원을 맞추기 위해 input data를 2^8 * 2^8로 만들고 학습시킴 -> 2^n 차원으로 축소

    이미지 색상 정보 = 채널
        흑백 = 1
        RGB = 3
    
    MLP 이미지 분석
        CNN은 사물의 위치가 달라지면 filtering이 다른 결과로 나올 수 있음
        과대적합 피하는 방법 -> 데이터 확장(이미지 증식)
        이미지 크기는 동일하게 하면서 늘리기, 반전 등 다양한 이미지 set을 만듬

    n_gram = 윈도우 : 얼마만큼의 data를 겹치게 만들 것인지
    sliding : 단어들이 겹치도록 하는 방법, 물리적으로 붙어있는 단어들을 같이 학습시키는 방법
        -> sliding 하지 않으면 겹치는 부분이 없어 이상하게 학습되기 때문에 sliding 필요
        -> stride size가 filter size보다 작아야 함

    전이학습
        pretrained model
        민감한 주제
        유통시킨 hidden layer를 받아 나의 data를 학습시키는 방법
        지금은 XAi 같은 것들 때문에 금지되어 있음
        그래서 hidden layer의 파라미터를 빼고 유통하는 방법을 개발 중 -> 파라미터를 빼도 예측을 잘한다고 함

RNN
    시계열
        기본적으로 1차 함수 사용
        시간, 언어, epoch, app로그, 커맨드
        -> 주가, 번역
    다음 단어를 쓰기 위해서는 이전 단어를 기억하고 있어야 한다
    데이터가 순서가 있기 때문에 hidden layer 안에서 data들 간 연결을 통해 output 에측
    
    시간은 모든 변수에 독립변수이다
    
    자기 회귀(automatic regression)
        과거의 data를 통해 현재의 data를 예측
        1차함수 사용

    이동평균(moving average)
        시계열 = 추세 + 주기 + 잔차
            추세 = 주기의 평균값을 구한 방향
            de-trend = 시계열 - 추세
            주기 = 같은 주기에 포함되어 있는 값들의 평균
            잔차만 예측
        

분석
선형성 직선에 위배되는 구간에 원인을 찾고 수학적 근거로 증거를 보여줌