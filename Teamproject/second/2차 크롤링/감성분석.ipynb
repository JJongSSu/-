{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class KnuSL():\n",
    "\n",
    "\tdef data_list(wordname):\t\n",
    "\t\twith open('../data/SentiWord_info2.json', encoding='utf-8-sig', mode='r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\tresult = ['None','None']\t\n",
    "\t\tfor i in range(0, len(data)):\n",
    "\t\t\tif data[i]['word'] == wordname:\n",
    "\t\t\t\tresult.pop()\n",
    "\t\t\t\tresult.pop()\n",
    "\t\t\t\tresult.append(data[i]['word_root'])\n",
    "\t\t\t\tresult.append(data[i]['polarity'])\t\n",
    "\t\t\n",
    "\t\tr_word = result[0]\n",
    "\t\ts_word = result[1]\n",
    "\t\t\t\t\t\t\t\n",
    "\t\tprint('어근 : ' + r_word)\n",
    "\t\tprint('극성 : ' + s_word)\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\treturn r_word, s_word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t\n",
    "\tksl = KnuSL\n",
    "\t\n",
    "\tprint(\"\\nKNU 한국어 감성사전입니다~ :)\")\n",
    "\tprint(\"사전에 단어가 없는 경우 결과가 None으로 나타납니다!!!\")\n",
    "\tprint(\"종료하시려면 #을 입력해주세요!!!\")\n",
    "\tprint(\"-2:매우 부정, -1:부정, 0:중립 or Unkwon, 1:긍정, 2:매우 긍정\")\n",
    "\tprint(\"\\n\")\t\n",
    "\n",
    "\twhile(True):\n",
    "\t\twordname = input(\"word : \")\n",
    "\t\twordname = wordname.strip(\" \")\t\t\n",
    "\t\tif wordname != \"#\":\n",
    "\t\t\tprint(ksl.data_list(wordname))\n",
    "\t\t\tprint(\"\\n\")\t\n",
    "\t\t\t\t\n",
    "\n",
    "\t\telif wordname == \"#\":\n",
    "\t\t\tprint(\"\\n이용해주셔서 감사합니다~ :)\")\n",
    "\t\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class KnuSL():\n",
    "\n",
    "\tdef data_list(wordname):\t\n",
    "\t\twith open('../data/SentiWord_info2.json', encoding='utf-8-sig', mode='r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\tresult = ['None','None']\t\n",
    "\t\tfor i in range(0, len(data)):\n",
    "\t\t\tif data[i]['word'] == wordname:\n",
    "\t\t\t\tresult.pop()\n",
    "\t\t\t\tresult.pop()\n",
    "\t\t\t\tresult.append(data[i]['word_root'])\n",
    "\t\t\t\tresult.append(data[i]['polarity'])\t\n",
    "\t\t\n",
    "\t\tr_word = result[0]\n",
    "\t\ts_word = result[1]\n",
    "\t\t\t\t\t\t\t\n",
    "\t\tprint('어근 : ' + r_word)\n",
    "\t\tprint('극성 : ' + s_word)\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\treturn r_word, s_word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t\n",
    "\tksl = KnuSL\n",
    "\n",
    "\twhile(True):\n",
    "\t\twordname = input(\"word : \")\n",
    "\t\twordname = wordname.strip(\" \")\t\t\n",
    "\t\tif wordname != \"#\":\n",
    "\t\t\tprint(ksl.data_list(wordname))\n",
    "\t\t\tprint(\"\\n\")\t\n",
    "\t\t\t\t\n",
    "\n",
    "\t\telif wordname == \"#\":\n",
    "\t\t\tprint(\"\\n이용해주셔서 감사합니다~ :)\")\n",
    "\t\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(self):\n",
    "        \"\"\"\n",
    "        Final output--to be displayed on the app\n",
    "        Columns: documents, top topic, n top words for topic, 2 corresponding sentences per word?\n",
    "        \"\"\"\n",
    "        file_to_tokens = self._get_normalized_corpus(self.files)\n",
    "        all_lemmas = self._get_all_unique_lemmas(file_to_tokens)\n",
    "\n",
    "        doc_topic = self.lda_model.doc_topic_  # document-topic distributions\n",
    "        topic_word = self.lda_model.topic_word_  # topic-word distributions\n",
    "\n",
    "        # Map lemmas to tokens\n",
    "        lemma_to_tokens = self._get_lemma_to_tokens(file_to_tokens)\n",
    "\n",
    "        # Map topic indices to lemmas\n",
    "        topic_lemmas_map = {}\n",
    "        for i, topic_dist in enumerate(topic_word):\n",
    "            top_lemmas = list(np.array(all_lemmas)[np.argsort(topic_dist)][\n",
    "                : -self.n_top_words : -1\n",
    "            ])  # list of top lemmas\n",
    "            top_lemmas_to_tokens = {}\n",
    "            for lemma in top_lemmas:\n",
    "                top_tokens = lemma_to_tokens[lemma]  # Grab all tokens corresponding to that lemma\n",
    "                top_lemmas_to_tokens[lemma] = top_tokens\n",
    "            topic_lemmas_map[i] = top_lemmas_to_tokens\n",
    "\n",
    "        # Final output: a map of doc_names to topic number, lemmas assoc. with that topic\n",
    "        # and the tokens associated with those words\n",
    "        doc_topic_words = {}\n",
    "        for i in range(len(self.files)):\n",
    "            top_topic = doc_topic[i].argmax()\n",
    "            current_file = self.files[i]\n",
    "            doc_topic_words[self.files[i]] = {top_topic: topic_lemmas_map[top_topic]}\n",
    "\n",
    "        self._doc_topic_words = doc_topic_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a926afa313b26ae1264fdcf81c726a97e69f6ba2ba780f6aa901948710f8d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
